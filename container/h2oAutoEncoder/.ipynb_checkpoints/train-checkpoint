#!/usr/bin/env python

# A sample training component that trains a simple scikit-learn decision tree model.
# This implementation works in File mode and makes no assumptions about the input file names.
# Input is specified as CSV with a data point in each row and the labels in the first column.

from __future__ import print_function

import os
import json
import pickle
import sys
import traceback
import pandas as pd
import numpy as np
import h2o
from sklearn.preprocessing import StandardScaler,MinMaxScaler,RobustScaler,Normalizer
from h2o.estimators.deeplearning import H2OAutoEncoderEstimator

from sklearn import tree

# These are the paths to where SageMaker mounts interesting things in your container.

prefix = '/opt/ml/'

input_path = prefix + 'input/data'
output_path = os.path.join(prefix, 'output')
model_path = os.path.join(prefix, 'model')
param_path = os.path.join(prefix, 'input/config/hyperparameters.json')

# This algorithm has a single channel of input data called 'training'. Since we run in
# File mode, the input files are copied to the directory specified here.
channel_name='training'
training_path = os.path.join(input_path, channel_name)

# The function to execute the training.
input_files = []
print(os.listdir(training_path))
def train():
    print('Starting the training.')
    try:
        # Read in any hyperparameters that the user passed with the training job
        with open(param_path, 'r') as tc:
            trainingParams = json.load(tc)
            print(trainingParams)
        # Take the set of files and read them all into a single pandas dataframe
        # input_files = [ os.path.join(training_path, file) for file in os.listdir(training_path) ]
        for path, subdirs, files in os.walk(training_path):
            for file in files:
                input_files.append(os.path.join(path, file))
        print(input_files)       
        if len(input_files) == 0:
            raise ValueError(('There are no files in {}.\n' +
                              'This usually indicates that the channel ({}) was incorrectly specified,\n' +
                              'the data specification in S3 was incorrectly specified or the role specified\n' +
                              'does not have permission to access the data.').format(training_path, channel_name))
        #raw_data = [ pd.read_csv(file, header=None, engine='python') for file in input_files ]
        #train_data = pd.concat(raw_data)
        h2o.init(nthreads=-1, enable_assertions = False)
        for file in input_files:
            student = pd.read_csv(file, sep=',')
            print("printing students")
            print(student)
            student.isnull().any()
            student=student.fillna(0)
            predictors=list(range(0,15))
            cols_to_transform = [ 'continue_drop','gender','caste','guardian','internet' ]
            student = pd.get_dummies( student,columns = cols_to_transform )
            student = student.drop('student_id', 1)
            # Copy the original dataset
            scaled_features = student.copy()
            
            # Extract column names to be standardized
            col_names = ['mathematics_marks','english_marks','science_marks',
                         'science_teacher','languages_teacher','school_id',
                         'total_students','total_toilets','establishment_year']
            
            # Standardize the columns and re-assingn to original dataframe
            features = scaled_features[col_names]
            scaler = RobustScaler().fit_transform(features.values)
            features = pd.DataFrame(scaler, index=student.index, columns=col_names)
            scaled_features [col_names] = features
            scaled_features.head()
            train=scaled_features.loc[scaled_features['continue_drop_continue'] == 1]
            test=scaled_features.loc[scaled_features['continue_drop_drop'] == 1]
            print(train)
            print("+++++++++++")
            train.hex=h2o.H2OFrame(train)
            test.hex=h2o.H2OFrame(test)
            model=H2OAutoEncoderEstimator(activation="Tanh",hidden=[120],ignore_const_cols=False,epochs=100)
            model.train(x=predictors,training_frame=train.hex)
            
            #saving the model artifacts
            model_location = h2o.save_model(model=model, path=model_path + file, force=True)
            print('Saving the model artifacts in ' + model_location)
        print('Training complete.')
        
    except Exception as e:
        # DescribeTrainingJob result.
        trc = traceback.format_exc()
        with open(os.path.join(output_path, 'failure'), 'w') as s:
            s.write('Exception during training: ' + str(e) + '\n' + trc)
        # Printing this causes the exception to be in the training job logs, as well.
        print('Exception during training: ' + str(e) + '\n' + trc, file=sys.stderr)
        # A non-zero exit code causes the training job to be marked as Failed.
        sys.exit(255)
        
if __name__ == '__main__':
    train()
    # A zero exit code causes the job to be marked a Succeeded.
    sys.exit(0)
